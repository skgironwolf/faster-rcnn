+ set -e
+ export PYTHONUNBUFFERED=True
+ PYTHONUNBUFFERED=True
+ GPU_ID=1
+ NET=ZF
+ NET_lc=zf
+ DATASET=cnn_data
+ array=($@)
+ len=3
+ EXTRA_ARGS=
+ EXTRA_ARGS_SLUG=
+ case $DATASET in
+ TRAIN_IMDB=CNNDATA_train
+ TEST_IMDB=CNNDATA_test
+ PT_DIR=pascal_voc
+ ITERS=0
++ date +%Y-%m-%d_%H-%M-%S
+ LOG=experiments/logs/faster_rcnn_end2end_ZF_.txt.2017-04-30_12-05-20
+ exec
++ tee -a experiments/logs/faster_rcnn_end2end_ZF_.txt.2017-04-30_12-05-20
tee: experiments/logs/faster_rcnn_end2end_ZF_.txt.2017-04-30_12-05-20: No such file or directory
+ echo Logging output to experiments/logs/faster_rcnn_end2end_ZF_.txt.2017-04-30_12-05-20
Logging output to experiments/logs/faster_rcnn_end2end_ZF_.txt.2017-04-30_12-05-20
+ ../../tools/test_net.py --gpu 1 --def ../../models/pascal_voc/ZF/faster_rcnn_end2end/test.prototxt --net ../../output/default/voc_2007_trainval/zf_faster_rcnn_iter_80000.caffemodel --imdb CNNDATA_test --cfg ../../experiments/cfgs/roost.yml
<function <lambda> at 0x2aaad9d07e60>
<function <lambda> at 0x2aaad9d07ed8>
<function <lambda> at 0x2aaad9d07f50>
<function <lambda> at 0x2aaad9d0c050>
Called with args:
Namespace(caffemodel='../../output/default/voc_2007_trainval/zf_faster_rcnn_iter_80000.caffemodel', cfg_file='../../experiments/cfgs/roost.yml', comp_mode=False, gpu_id=1, imdb_name='CNNDATA_test', max_per_image=100, prototxt='../../models/pascal_voc/ZF/faster_rcnn_end2end/test.prototxt', set_cfgs=None, vis=False, wait=True)
Using config:
{'DATA_DIR': '/home/sgabriel/py-faster-rcnn/data',
 'DEDUP_BOXES': 0.0625,
 'EPS': 1e-14,
 'EXP_DIR': 'faster_rcnn_end2end',
 'GPU_ID': 1,
 'MATLAB': 'matlab',
 'MODELS_DIR': '/home/sgabriel/py-faster-rcnn/models/pascal_voc',
 'PIXEL_MEANS': array([[[ 102.9801,  115.9465,  122.7717]]]),
 'RNG_SEED': 17,
 'ROOT_DIR': '/home/sgabriel/py-faster-rcnn',
 'TEST': {'BBOX_REG': False,
          'HAS_RPN': True,
          'MAX_SIZE': 600,
          'NMS': 0.3,
          'PROPOSAL_METHOD': 'selective_search',
          'RPN_MIN_SIZE': 16,
          'RPN_NMS_THRESH': 0.7,
          'RPN_POST_NMS_TOP_N': 300,
          'RPN_PRE_NMS_TOP_N': 6000,
          'SCALES': [600],
          'SVM': False},
 'TRAIN': {'ASPECT_GROUPING': True,
           'BATCH_SIZE': 4,
           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],
           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],
           'BBOX_NORMALIZE_TARGETS': True,
           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,
           'BBOX_REG': False,
           'BBOX_THRESH': 0.5,
           'BG_THRESH_HI': 0.5,
           'BG_THRESH_LO': 0.0,
           'FG_FRACTION': 0.25,
           'FG_THRESH': 0.5,
           'HAS_RPN': True,
           'IMS_PER_BATCH': 1,
           'MAX_SIZE': 600,
           'PROPOSAL_METHOD': 'gt',
           'RPN_BATCHSIZE': 256,
           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],
           'RPN_CLOBBER_POSITIVES': False,
           'RPN_FG_FRACTION': 0.5,
           'RPN_MIN_SIZE': 16,
           'RPN_NEGATIVE_OVERLAP': 0.3,
           'RPN_NMS_THRESH': 0.7,
           'RPN_POSITIVE_OVERLAP': 0.7,
           'RPN_POSITIVE_WEIGHT': -1.0,
           'RPN_POST_NMS_TOP_N': 2000,
           'RPN_PRE_NMS_TOP_N': 12000,
           'SCALES': [600],
           'SNAPSHOT_INFIX': '',
           'SNAPSHOT_ITERS': 100000,
           'USE_FLIPPED': True,
           'USE_PREFETCH': False},
 'USE_GPU_NMS': False}
../../tools/test_net.py:80: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  caffe.set_mode_gpu()
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0430 12:05:23.622313 14177 net.cpp:49] Initializing net from parameters: 
name: "ZF"
input: "data"
input: "im_info"
state {
  phase: TEST
}
input_shape {
  dim: 1
  dim: 3
  dim: 224
  dim: 224
}
input_shape {
  dim: 1
  dim: 3
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    pad: 3
    kernel_size: 7
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "conv1"
  top: "norm1"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "norm1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    stride: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "conv2"
  top: "norm2"
  lrn_param {
    local_size: 3
    alpha: 5e-05
    beta: 0.75
    norm_region: WITHIN_CHANNEL
    engine: CAFFE
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "norm2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    pad: 1
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "rpn_conv/3x3"
  type: "Convolution"
  bottom: "conv5"
  top: "rpn/output"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_relu/3x3"
  type: "ReLU"
  bottom: "rpn/output"
  top: "rpn/output"
}
layer {
  name: "rpn_cls_score"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_cls_score"
  convolution_param {
    num_output: 18
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_bbox_pred"
  type: "Convolution"
  bottom: "rpn/output"
  top: "rpn_bbox_pred"
  convolution_param {
    num_output: 36
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "rpn_cls_score_reshape"
  type: "Reshape"
  bottom: "rpn_cls_score"
  top: "rpn_cls_score_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 2
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "rpn_cls_prob"
  type: "Softmax"
  bottom: "rpn_cls_score_reshape"
  top: "rpn_cls_prob"
}
layer {
  name: "rpn_cls_prob_reshape"
  type: "Reshape"
  bottom: "rpn_cls_prob"
  top: "rpn_cls_prob_reshape"
  reshape_param {
    shape {
      dim: 0
      dim: 18
      dim: -1
      dim: 0
    }
  }
}
layer {
  name: "proposal"
  type: "Python"
  bottom: "rpn_cls_prob_reshape"
  bottom: "rpn_bbox_pred"
  bottom: "im_info"
  top: "rois"
  python_param {
    module: "rpn.proposal_layer"
    layer: "ProposalLayer"
    param_str: "\'feat_stride\': 16"
  }
}
layer {
  name: "roi_pool_conv5"
  type: "ROIPooling"
  bottom: "conv5"
  bottom: "rois"
  top: "roi_pool_conv5"
  roi_pooling_param {
    pooled_h: 6
    pooled_w: 6
    spatial_scale: 0.0625
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "roi_pool_conv5"
  top: "fc6"
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
    scale_train: false
  }
}
layer {
  name: "cls_score"
  type: "InnerProduct"
  bottom: "fc7"
  top: "cls_score"
  inner_product_param {
    num_output: 2
  }
}
layer {
  name: "bbox_pred"
  type: "InnerProduct"
  bottom: "fc7"
  top: "bbox_pred"
  inner_product_param {
    num_output: 8
  }
}
layer {
  name: "cls_prob"
  type: "Softmax"
  bottom: "cls_score"
  top: "cls_prob"
  loss_param {
    ignore_label: -1
    normalize: true
  }
}
I0430 12:05:23.622475 14177 net.cpp:413] Input 0 -> data
I0430 12:05:23.645951 14177 net.cpp:413] Input 1 -> im_info
I0430 12:05:23.646006 14177 layer_factory.hpp:77] Creating layer conv1
I0430 12:05:23.646031 14177 net.cpp:106] Creating Layer conv1
I0430 12:05:23.646037 14177 net.cpp:454] conv1 <- data
I0430 12:05:23.646042 14177 net.cpp:411] conv1 -> conv1
I0430 12:05:23.646880 14177 net.cpp:150] Setting up conv1
I0430 12:05:23.646895 14177 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I0430 12:05:23.646899 14177 net.cpp:165] Memory required for data: 4816896
I0430 12:05:23.646914 14177 layer_factory.hpp:77] Creating layer relu1
I0430 12:05:23.646929 14177 net.cpp:106] Creating Layer relu1
I0430 12:05:23.646935 14177 net.cpp:454] relu1 <- conv1
I0430 12:05:23.646942 14177 net.cpp:397] relu1 -> conv1 (in-place)
I0430 12:05:23.646955 14177 net.cpp:150] Setting up relu1
I0430 12:05:23.646961 14177 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I0430 12:05:23.646965 14177 net.cpp:165] Memory required for data: 9633792
I0430 12:05:23.646971 14177 layer_factory.hpp:77] Creating layer norm1
I0430 12:05:23.646983 14177 net.cpp:106] Creating Layer norm1
I0430 12:05:23.646987 14177 net.cpp:454] norm1 <- conv1
I0430 12:05:23.646996 14177 net.cpp:411] norm1 -> norm1
I0430 12:05:23.647104 14177 net.cpp:150] Setting up norm1
I0430 12:05:23.647112 14177 net.cpp:157] Top shape: 1 96 112 112 (1204224)
I0430 12:05:23.647116 14177 net.cpp:165] Memory required for data: 14450688
I0430 12:05:23.647121 14177 layer_factory.hpp:77] Creating layer pool1
I0430 12:05:23.647130 14177 net.cpp:106] Creating Layer pool1
I0430 12:05:23.647135 14177 net.cpp:454] pool1 <- norm1
I0430 12:05:23.647145 14177 net.cpp:411] pool1 -> pool1
I0430 12:05:23.647182 14177 net.cpp:150] Setting up pool1
I0430 12:05:23.647189 14177 net.cpp:157] Top shape: 1 96 57 57 (311904)
I0430 12:05:23.647194 14177 net.cpp:165] Memory required for data: 15698304
I0430 12:05:23.647199 14177 layer_factory.hpp:77] Creating layer conv2
I0430 12:05:23.647209 14177 net.cpp:106] Creating Layer conv2
I0430 12:05:23.647213 14177 net.cpp:454] conv2 <- pool1
I0430 12:05:23.647222 14177 net.cpp:411] conv2 -> conv2
I0430 12:05:23.649603 14177 net.cpp:150] Setting up conv2
I0430 12:05:23.649617 14177 net.cpp:157] Top shape: 1 256 29 29 (215296)
I0430 12:05:23.649623 14177 net.cpp:165] Memory required for data: 16559488
I0430 12:05:23.649636 14177 layer_factory.hpp:77] Creating layer relu2
I0430 12:05:23.649644 14177 net.cpp:106] Creating Layer relu2
I0430 12:05:23.649649 14177 net.cpp:454] relu2 <- conv2
I0430 12:05:23.649658 14177 net.cpp:397] relu2 -> conv2 (in-place)
I0430 12:05:23.649667 14177 net.cpp:150] Setting up relu2
I0430 12:05:23.649673 14177 net.cpp:157] Top shape: 1 256 29 29 (215296)
I0430 12:05:23.649677 14177 net.cpp:165] Memory required for data: 17420672
I0430 12:05:23.649682 14177 layer_factory.hpp:77] Creating layer norm2
I0430 12:05:23.649690 14177 net.cpp:106] Creating Layer norm2
I0430 12:05:23.649693 14177 net.cpp:454] norm2 <- conv2
I0430 12:05:23.649700 14177 net.cpp:411] norm2 -> norm2
I0430 12:05:23.649811 14177 net.cpp:150] Setting up norm2
I0430 12:05:23.649821 14177 net.cpp:157] Top shape: 1 256 29 29 (215296)
I0430 12:05:23.649826 14177 net.cpp:165] Memory required for data: 18281856
I0430 12:05:23.649830 14177 layer_factory.hpp:77] Creating layer pool2
I0430 12:05:23.649840 14177 net.cpp:106] Creating Layer pool2
I0430 12:05:23.649844 14177 net.cpp:454] pool2 <- norm2
I0430 12:05:23.649852 14177 net.cpp:411] pool2 -> pool2
I0430 12:05:23.649888 14177 net.cpp:150] Setting up pool2
I0430 12:05:23.649894 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.649900 14177 net.cpp:165] Memory required for data: 18512256
I0430 12:05:23.649905 14177 layer_factory.hpp:77] Creating layer conv3
I0430 12:05:23.649915 14177 net.cpp:106] Creating Layer conv3
I0430 12:05:23.649920 14177 net.cpp:454] conv3 <- pool2
I0430 12:05:23.649933 14177 net.cpp:411] conv3 -> conv3
I0430 12:05:23.652842 14177 net.cpp:150] Setting up conv3
I0430 12:05:23.652858 14177 net.cpp:157] Top shape: 1 384 15 15 (86400)
I0430 12:05:23.652863 14177 net.cpp:165] Memory required for data: 18857856
I0430 12:05:23.652876 14177 layer_factory.hpp:77] Creating layer relu3
I0430 12:05:23.652887 14177 net.cpp:106] Creating Layer relu3
I0430 12:05:23.652892 14177 net.cpp:454] relu3 <- conv3
I0430 12:05:23.652900 14177 net.cpp:397] relu3 -> conv3 (in-place)
I0430 12:05:23.652909 14177 net.cpp:150] Setting up relu3
I0430 12:05:23.652915 14177 net.cpp:157] Top shape: 1 384 15 15 (86400)
I0430 12:05:23.652920 14177 net.cpp:165] Memory required for data: 19203456
I0430 12:05:23.652927 14177 layer_factory.hpp:77] Creating layer conv4
I0430 12:05:23.652940 14177 net.cpp:106] Creating Layer conv4
I0430 12:05:23.652943 14177 net.cpp:454] conv4 <- conv3
I0430 12:05:23.652951 14177 net.cpp:411] conv4 -> conv4
I0430 12:05:23.656476 14177 net.cpp:150] Setting up conv4
I0430 12:05:23.656491 14177 net.cpp:157] Top shape: 1 384 15 15 (86400)
I0430 12:05:23.656496 14177 net.cpp:165] Memory required for data: 19549056
I0430 12:05:23.656505 14177 layer_factory.hpp:77] Creating layer relu4
I0430 12:05:23.656513 14177 net.cpp:106] Creating Layer relu4
I0430 12:05:23.656518 14177 net.cpp:454] relu4 <- conv4
I0430 12:05:23.656525 14177 net.cpp:397] relu4 -> conv4 (in-place)
I0430 12:05:23.656534 14177 net.cpp:150] Setting up relu4
I0430 12:05:23.656540 14177 net.cpp:157] Top shape: 1 384 15 15 (86400)
I0430 12:05:23.656544 14177 net.cpp:165] Memory required for data: 19894656
I0430 12:05:23.656549 14177 layer_factory.hpp:77] Creating layer conv5
I0430 12:05:23.656560 14177 net.cpp:106] Creating Layer conv5
I0430 12:05:23.656564 14177 net.cpp:454] conv5 <- conv4
I0430 12:05:23.656574 14177 net.cpp:411] conv5 -> conv5
I0430 12:05:23.658946 14177 net.cpp:150] Setting up conv5
I0430 12:05:23.658962 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.658978 14177 net.cpp:165] Memory required for data: 20125056
I0430 12:05:23.658991 14177 layer_factory.hpp:77] Creating layer relu5
I0430 12:05:23.658999 14177 net.cpp:106] Creating Layer relu5
I0430 12:05:23.659003 14177 net.cpp:454] relu5 <- conv5
I0430 12:05:23.659011 14177 net.cpp:397] relu5 -> conv5 (in-place)
I0430 12:05:23.659019 14177 net.cpp:150] Setting up relu5
I0430 12:05:23.659025 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.659029 14177 net.cpp:165] Memory required for data: 20355456
I0430 12:05:23.659034 14177 layer_factory.hpp:77] Creating layer conv5_relu5_0_split
I0430 12:05:23.659047 14177 net.cpp:106] Creating Layer conv5_relu5_0_split
I0430 12:05:23.659051 14177 net.cpp:454] conv5_relu5_0_split <- conv5
I0430 12:05:23.659062 14177 net.cpp:411] conv5_relu5_0_split -> conv5_relu5_0_split_0
I0430 12:05:23.659070 14177 net.cpp:411] conv5_relu5_0_split -> conv5_relu5_0_split_1
I0430 12:05:23.659107 14177 net.cpp:150] Setting up conv5_relu5_0_split
I0430 12:05:23.659114 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.659122 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.659126 14177 net.cpp:165] Memory required for data: 20816256
I0430 12:05:23.659131 14177 layer_factory.hpp:77] Creating layer rpn_conv/3x3
I0430 12:05:23.659144 14177 net.cpp:106] Creating Layer rpn_conv/3x3
I0430 12:05:23.659148 14177 net.cpp:454] rpn_conv/3x3 <- conv5_relu5_0_split_0
I0430 12:05:23.659157 14177 net.cpp:411] rpn_conv/3x3 -> rpn/output
I0430 12:05:23.670584 14177 net.cpp:150] Setting up rpn_conv/3x3
I0430 12:05:23.670600 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.670613 14177 net.cpp:165] Memory required for data: 21046656
I0430 12:05:23.670624 14177 layer_factory.hpp:77] Creating layer rpn_relu/3x3
I0430 12:05:23.670636 14177 net.cpp:106] Creating Layer rpn_relu/3x3
I0430 12:05:23.670641 14177 net.cpp:454] rpn_relu/3x3 <- rpn/output
I0430 12:05:23.670650 14177 net.cpp:397] rpn_relu/3x3 -> rpn/output (in-place)
I0430 12:05:23.670658 14177 net.cpp:150] Setting up rpn_relu/3x3
I0430 12:05:23.670665 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.670668 14177 net.cpp:165] Memory required for data: 21277056
I0430 12:05:23.670675 14177 layer_factory.hpp:77] Creating layer rpn/output_rpn_relu/3x3_0_split
I0430 12:05:23.670681 14177 net.cpp:106] Creating Layer rpn/output_rpn_relu/3x3_0_split
I0430 12:05:23.670686 14177 net.cpp:454] rpn/output_rpn_relu/3x3_0_split <- rpn/output
I0430 12:05:23.670693 14177 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_0
I0430 12:05:23.670702 14177 net.cpp:411] rpn/output_rpn_relu/3x3_0_split -> rpn/output_rpn_relu/3x3_0_split_1
I0430 12:05:23.670737 14177 net.cpp:150] Setting up rpn/output_rpn_relu/3x3_0_split
I0430 12:05:23.670744 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.670752 14177 net.cpp:157] Top shape: 1 256 15 15 (57600)
I0430 12:05:23.670755 14177 net.cpp:165] Memory required for data: 21737856
I0430 12:05:23.670760 14177 layer_factory.hpp:77] Creating layer rpn_cls_score
I0430 12:05:23.670773 14177 net.cpp:106] Creating Layer rpn_cls_score
I0430 12:05:23.670778 14177 net.cpp:454] rpn_cls_score <- rpn/output_rpn_relu/3x3_0_split_0
I0430 12:05:23.670788 14177 net.cpp:411] rpn_cls_score -> rpn_cls_score
I0430 12:05:23.671072 14177 net.cpp:150] Setting up rpn_cls_score
I0430 12:05:23.671082 14177 net.cpp:157] Top shape: 1 18 15 15 (4050)
I0430 12:05:23.671087 14177 net.cpp:165] Memory required for data: 21754056
I0430 12:05:23.671097 14177 layer_factory.hpp:77] Creating layer rpn_bbox_pred
I0430 12:05:23.671108 14177 net.cpp:106] Creating Layer rpn_bbox_pred
I0430 12:05:23.671113 14177 net.cpp:454] rpn_bbox_pred <- rpn/output_rpn_relu/3x3_0_split_1
I0430 12:05:23.671121 14177 net.cpp:411] rpn_bbox_pred -> rpn_bbox_pred
I0430 12:05:23.671469 14177 net.cpp:150] Setting up rpn_bbox_pred
I0430 12:05:23.671478 14177 net.cpp:157] Top shape: 1 36 15 15 (8100)
I0430 12:05:23.671483 14177 net.cpp:165] Memory required for data: 21786456
I0430 12:05:23.671492 14177 layer_factory.hpp:77] Creating layer rpn_cls_score_reshape
I0430 12:05:23.671509 14177 net.cpp:106] Creating Layer rpn_cls_score_reshape
I0430 12:05:23.671515 14177 net.cpp:454] rpn_cls_score_reshape <- rpn_cls_score
I0430 12:05:23.671524 14177 net.cpp:411] rpn_cls_score_reshape -> rpn_cls_score_reshape
I0430 12:05:23.671555 14177 net.cpp:150] Setting up rpn_cls_score_reshape
I0430 12:05:23.671562 14177 net.cpp:157] Top shape: 1 2 135 15 (4050)
I0430 12:05:23.671567 14177 net.cpp:165] Memory required for data: 21802656
I0430 12:05:23.671571 14177 layer_factory.hpp:77] Creating layer rpn_cls_prob
I0430 12:05:23.671579 14177 net.cpp:106] Creating Layer rpn_cls_prob
I0430 12:05:23.671584 14177 net.cpp:454] rpn_cls_prob <- rpn_cls_score_reshape
I0430 12:05:23.671593 14177 net.cpp:411] rpn_cls_prob -> rpn_cls_prob
I0430 12:05:23.671648 14177 net.cpp:150] Setting up rpn_cls_prob
I0430 12:05:23.671656 14177 net.cpp:157] Top shape: 1 2 135 15 (4050)
I0430 12:05:23.671661 14177 net.cpp:165] Memory required for data: 21818856
I0430 12:05:23.671666 14177 layer_factory.hpp:77] Creating layer rpn_cls_prob_reshape
I0430 12:05:23.671675 14177 net.cpp:106] Creating Layer rpn_cls_prob_reshape
I0430 12:05:23.671680 14177 net.cpp:454] rpn_cls_prob_reshape <- rpn_cls_prob
I0430 12:05:23.671687 14177 net.cpp:411] rpn_cls_prob_reshape -> rpn_cls_prob_reshape
I0430 12:05:23.671712 14177 net.cpp:150] Setting up rpn_cls_prob_reshape
I0430 12:05:23.671718 14177 net.cpp:157] Top shape: 1 18 15 15 (4050)
I0430 12:05:23.671723 14177 net.cpp:165] Memory required for data: 21835056
I0430 12:05:23.671728 14177 layer_factory.hpp:77] Creating layer proposal
I0430 12:05:23.674644 14177 net.cpp:106] Creating Layer proposal
I0430 12:05:23.674657 14177 net.cpp:454] proposal <- rpn_cls_prob_reshape
I0430 12:05:23.674664 14177 net.cpp:454] proposal <- rpn_bbox_pred
I0430 12:05:23.674669 14177 net.cpp:454] proposal <- im_info
I0430 12:05:23.674675 14177 net.cpp:411] proposal -> rois
I0430 12:05:23.675689 14177 net.cpp:150] Setting up proposal
I0430 12:05:23.675704 14177 net.cpp:157] Top shape: 1 5 (5)
I0430 12:05:23.675707 14177 net.cpp:165] Memory required for data: 21835076
I0430 12:05:23.675711 14177 layer_factory.hpp:77] Creating layer roi_pool_conv5
I0430 12:05:23.675722 14177 net.cpp:106] Creating Layer roi_pool_conv5
I0430 12:05:23.675726 14177 net.cpp:454] roi_pool_conv5 <- conv5_relu5_0_split_1
I0430 12:05:23.675731 14177 net.cpp:454] roi_pool_conv5 <- rois
I0430 12:05:23.675736 14177 net.cpp:411] roi_pool_conv5 -> roi_pool_conv5
I0430 12:05:23.675747 14177 roi_pooling_layer.cpp:30] Spatial scale: 0.0625
I0430 12:05:23.675793 14177 net.cpp:150] Setting up roi_pool_conv5
I0430 12:05:23.675799 14177 net.cpp:157] Top shape: 1 256 6 6 (9216)
I0430 12:05:23.675802 14177 net.cpp:165] Memory required for data: 21871940
I0430 12:05:23.675806 14177 layer_factory.hpp:77] Creating layer fc6
I0430 12:05:23.675814 14177 net.cpp:106] Creating Layer fc6
I0430 12:05:23.675818 14177 net.cpp:454] fc6 <- roi_pool_conv5
I0430 12:05:23.675823 14177 net.cpp:411] fc6 -> fc6
I0430 12:05:23.779480 14177 net.cpp:150] Setting up fc6
I0430 12:05:23.779511 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.779515 14177 net.cpp:165] Memory required for data: 21888324
I0430 12:05:23.779534 14177 layer_factory.hpp:77] Creating layer relu6
I0430 12:05:23.779546 14177 net.cpp:106] Creating Layer relu6
I0430 12:05:23.779554 14177 net.cpp:454] relu6 <- fc6
I0430 12:05:23.779562 14177 net.cpp:397] relu6 -> fc6 (in-place)
I0430 12:05:23.779573 14177 net.cpp:150] Setting up relu6
I0430 12:05:23.779578 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.779582 14177 net.cpp:165] Memory required for data: 21904708
I0430 12:05:23.779585 14177 layer_factory.hpp:77] Creating layer drop6
I0430 12:05:23.779599 14177 net.cpp:106] Creating Layer drop6
I0430 12:05:23.779605 14177 net.cpp:454] drop6 <- fc6
I0430 12:05:23.779609 14177 net.cpp:397] drop6 -> fc6 (in-place)
I0430 12:05:23.779636 14177 net.cpp:150] Setting up drop6
I0430 12:05:23.779642 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.779645 14177 net.cpp:165] Memory required for data: 21921092
I0430 12:05:23.779649 14177 layer_factory.hpp:77] Creating layer fc7
I0430 12:05:23.779657 14177 net.cpp:106] Creating Layer fc7
I0430 12:05:23.779660 14177 net.cpp:454] fc7 <- fc6
I0430 12:05:23.779664 14177 net.cpp:411] fc7 -> fc7
I0430 12:05:23.826192 14177 net.cpp:150] Setting up fc7
I0430 12:05:23.826221 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.826225 14177 net.cpp:165] Memory required for data: 21937476
I0430 12:05:23.826236 14177 layer_factory.hpp:77] Creating layer relu7
I0430 12:05:23.826247 14177 net.cpp:106] Creating Layer relu7
I0430 12:05:23.826256 14177 net.cpp:454] relu7 <- fc7
I0430 12:05:23.826261 14177 net.cpp:397] relu7 -> fc7 (in-place)
I0430 12:05:23.826272 14177 net.cpp:150] Setting up relu7
I0430 12:05:23.826280 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.826287 14177 net.cpp:165] Memory required for data: 21953860
I0430 12:05:23.826292 14177 layer_factory.hpp:77] Creating layer drop7
I0430 12:05:23.826300 14177 net.cpp:106] Creating Layer drop7
I0430 12:05:23.826305 14177 net.cpp:454] drop7 <- fc7
I0430 12:05:23.826310 14177 net.cpp:397] drop7 -> fc7 (in-place)
I0430 12:05:23.826344 14177 net.cpp:150] Setting up drop7
I0430 12:05:23.826354 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.826359 14177 net.cpp:165] Memory required for data: 21970244
I0430 12:05:23.826361 14177 layer_factory.hpp:77] Creating layer fc7_drop7_0_split
I0430 12:05:23.826369 14177 net.cpp:106] Creating Layer fc7_drop7_0_split
I0430 12:05:23.826372 14177 net.cpp:454] fc7_drop7_0_split <- fc7
I0430 12:05:23.826378 14177 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_0
I0430 12:05:23.826385 14177 net.cpp:411] fc7_drop7_0_split -> fc7_drop7_0_split_1
I0430 12:05:23.826426 14177 net.cpp:150] Setting up fc7_drop7_0_split
I0430 12:05:23.826431 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.826436 14177 net.cpp:157] Top shape: 1 4096 (4096)
I0430 12:05:23.826439 14177 net.cpp:165] Memory required for data: 22003012
I0430 12:05:23.826442 14177 layer_factory.hpp:77] Creating layer cls_score
I0430 12:05:23.826457 14177 net.cpp:106] Creating Layer cls_score
I0430 12:05:23.826460 14177 net.cpp:454] cls_score <- fc7_drop7_0_split_0
I0430 12:05:23.826467 14177 net.cpp:411] cls_score -> cls_score
I0430 12:05:23.826575 14177 net.cpp:150] Setting up cls_score
I0430 12:05:23.826581 14177 net.cpp:157] Top shape: 1 2 (2)
I0430 12:05:23.826584 14177 net.cpp:165] Memory required for data: 22003020
I0430 12:05:23.826591 14177 layer_factory.hpp:77] Creating layer bbox_pred
I0430 12:05:23.826597 14177 net.cpp:106] Creating Layer bbox_pred
I0430 12:05:23.826601 14177 net.cpp:454] bbox_pred <- fc7_drop7_0_split_1
I0430 12:05:23.826608 14177 net.cpp:411] bbox_pred -> bbox_pred
I0430 12:05:23.827519 14177 net.cpp:150] Setting up bbox_pred
I0430 12:05:23.827531 14177 net.cpp:157] Top shape: 1 8 (8)
I0430 12:05:23.827534 14177 net.cpp:165] Memory required for data: 22003052
I0430 12:05:23.827543 14177 layer_factory.hpp:77] Creating layer cls_prob
I0430 12:05:23.827553 14177 net.cpp:106] Creating Layer cls_prob
I0430 12:05:23.827556 14177 net.cpp:454] cls_prob <- cls_score
I0430 12:05:23.827563 14177 net.cpp:411] cls_prob -> cls_prob
I0430 12:05:23.827618 14177 net.cpp:150] Setting up cls_prob
I0430 12:05:23.827625 14177 net.cpp:157] Top shape: 1 2 (2)
I0430 12:05:23.827628 14177 net.cpp:165] Memory required for data: 22003060
I0430 12:05:23.827631 14177 net.cpp:228] cls_prob does not need backward computation.
I0430 12:05:23.827636 14177 net.cpp:228] bbox_pred does not need backward computation.
I0430 12:05:23.827639 14177 net.cpp:228] cls_score does not need backward computation.
I0430 12:05:23.827643 14177 net.cpp:228] fc7_drop7_0_split does not need backward computation.
I0430 12:05:23.827646 14177 net.cpp:228] drop7 does not need backward computation.
I0430 12:05:23.827649 14177 net.cpp:228] relu7 does not need backward computation.
I0430 12:05:23.827653 14177 net.cpp:228] fc7 does not need backward computation.
I0430 12:05:23.827657 14177 net.cpp:228] drop6 does not need backward computation.
I0430 12:05:23.827661 14177 net.cpp:228] relu6 does not need backward computation.
I0430 12:05:23.827664 14177 net.cpp:228] fc6 does not need backward computation.
I0430 12:05:23.827668 14177 net.cpp:228] roi_pool_conv5 does not need backward computation.
I0430 12:05:23.827673 14177 net.cpp:228] proposal does not need backward computation.
I0430 12:05:23.827677 14177 net.cpp:228] rpn_cls_prob_reshape does not need backward computation.
I0430 12:05:23.827683 14177 net.cpp:228] rpn_cls_prob does not need backward computation.
I0430 12:05:23.827687 14177 net.cpp:228] rpn_cls_score_reshape does not need backward computation.
I0430 12:05:23.827692 14177 net.cpp:228] rpn_bbox_pred does not need backward computation.
I0430 12:05:23.827694 14177 net.cpp:228] rpn_cls_score does not need backward computation.
I0430 12:05:23.827699 14177 net.cpp:228] rpn/output_rpn_relu/3x3_0_split does not need backward computation.
I0430 12:05:23.827703 14177 net.cpp:228] rpn_relu/3x3 does not need backward computation.
I0430 12:05:23.827706 14177 net.cpp:228] rpn_conv/3x3 does not need backward computation.
I0430 12:05:23.827710 14177 net.cpp:228] conv5_relu5_0_split does not need backward computation.
I0430 12:05:23.827714 14177 net.cpp:228] relu5 does not need backward computation.
I0430 12:05:23.827718 14177 net.cpp:228] conv5 does not need backward computation.
I0430 12:05:23.827723 14177 net.cpp:228] relu4 does not need backward computation.
I0430 12:05:23.827725 14177 net.cpp:228] conv4 does not need backward computation.
I0430 12:05:23.827729 14177 net.cpp:228] relu3 does not need backward computation.
I0430 12:05:23.827733 14177 net.cpp:228] conv3 does not need backward computation.
I0430 12:05:23.827736 14177 net.cpp:228] pool2 does not need backward computation.
I0430 12:05:23.827740 14177 net.cpp:228] norm2 does not need backward computation.
I0430 12:05:23.827744 14177 net.cpp:228] relu2 does not need backward computation.
I0430 12:05:23.827747 14177 net.cpp:228] conv2 does not need backward computation.
I0430 12:05:23.827751 14177 net.cpp:228] pool1 does not need backward computation.
I0430 12:05:23.827754 14177 net.cpp:228] norm1 does not need backward computation.
I0430 12:05:23.827759 14177 net.cpp:228] relu1 does not need backward computation.
I0430 12:05:23.827761 14177 net.cpp:228] conv1 does not need backward computation.
I0430 12:05:23.827765 14177 net.cpp:270] This network produces output bbox_pred
I0430 12:05:23.827769 14177 net.cpp:270] This network produces output cls_prob
I0430 12:05:23.827793 14177 net.cpp:283] Network initialization done.
I0430 12:05:26.105368 14177 net.cpp:816] Ignoring source layer input-data
I0430 12:05:26.105398 14177 net.cpp:816] Ignoring source layer data_input-data_0_split
I0430 12:05:26.105404 14177 net.cpp:816] Ignoring source layer im_info_input-data_1_split
I0430 12:05:26.105409 14177 net.cpp:816] Ignoring source layer gt_boxes_input-data_2_split
I0430 12:05:26.112478 14177 net.cpp:816] Ignoring source layer rpn_cls_score_rpn_cls_score_0_split
I0430 12:05:26.112527 14177 net.cpp:816] Ignoring source layer rpn_bbox_pred_rpn_bbox_pred_0_split
I0430 12:05:26.112532 14177 net.cpp:816] Ignoring source layer rpn_cls_score_reshape_rpn_cls_score_reshape_0_split
I0430 12:05:26.112536 14177 net.cpp:816] Ignoring source layer rpn-data
I0430 12:05:26.112538 14177 net.cpp:816] Ignoring source layer rpn_loss_cls
I0430 12:05:26.112542 14177 net.cpp:816] Ignoring source layer rpn_loss_bbox
I0430 12:05:26.112548 14177 net.cpp:816] Ignoring source layer roi-data
I0430 12:05:26.186655 14177 net.cpp:816] Ignoring source layer loss_cls
I0430 12:05:26.186681 14177 net.cpp:816] Ignoring source layer loss_bbox
NET
<caffe._caffe.Net object at 0x2aaada1aae10>
im_detect: 1/38 0.215s 0.000s
im_detect: 2/38 0.166s 0.000s
im_detect: 3/38 0.149s 0.000s
im_detect: 4/38 0.141s 0.000s
im_detect: 5/38 0.136s 0.000s
im_detect: 6/38 0.135s 0.000s
im_detect: 7/38 0.131s 0.000s
im_detect: 8/38 0.134s 0.000s
im_detect: 9/38 0.135s 0.000s
im_detect: 10/38 0.133s 0.000s
im_detect: 11/38 0.131s 0.000s
im_detect: 12/38 0.130s 0.000s
im_detect: 13/38 0.131s 0.000s
im_detect: 14/38 0.133s 0.000s
im_detect: 15/38 0.132s 0.000s
im_detect: 16/38 0.131s 0.000s
im_detect: 17/38 0.132s 0.000s
im_detect: 18/38 0.132s 0.000s
im_detect: 19/38 0.132s 0.000s
im_detect: 20/38 0.131s 0.000s
im_detect: 21/38 0.131s 0.000s
im_detect: 22/38 0.131s 0.000s
im_detect: 23/38 0.131s 0.000s
im_detect: 24/38 0.130s 0.000s
im_detect: 25/38 0.130s 0.000s
im_detect: 26/38 0.130s 0.000s
im_detect: 27/38 0.131s 0.000s
im_detect: 28/38 0.131s 0.000s
im_detect: 29/38 0.131s 0.000s
im_detect: 30/38 0.130s 0.000s
im_detect: 31/38 0.130s 0.000s
im_detect: 32/38 0.130s 0.000s
im_detect: 33/38 0.130s 0.000s
im_detect: 34/38 0.130s 0.000s
im_detect: 35/38 0.129s 0.000s
im_detect: 36/38 0.129s 0.000s
im_detect: 37/38 0.129s 0.000s
im_detect: 38/38 0.129s 0.000s
Evaluating detections
Writing roost VOC results file
VOC07 metric? Yes
/home/sgabriel/py-faster-rcnn/data/VOCdevkit2007/annotations_cache
5
12
20
27
34
38
40
41
50
59
68
76
82
83
84
85
86
87
88
89
90
91
92
93
95
97
102
107
109
112
115
119
120
127
134
136
139
141
2049
1.10911551692
1.20126343411
1.12822256808
0.903356364219
1.38376106554
0.950790375861
1.144408569
1.07929445063
0.0
0.812596599691
0.847822897179
0.76272610517
0.0
0.0
0.754137900441
1.21562541889
0.0
0.0
0.0
0.55181667428
0.0
1.02505192752
0.542641563061
0.255994054506
0.829951881643
1.30158730159
1.32734381827
1.23696902633
0.584385449565
0.692815759932
0.0368499335078
1.15970902418
0.18923633908
0.0
0.917884268247
0.0
1.1236079649
0.836145114623
0.0
0.0
0.0
0.0
0.0
0.0
0.939919818756
0.487978340608
0.0
1.12936905875
0.0
0.0
0.0
0.776794766372
0.0
0.0
0.0
0.0
0.62063264489
1.17696654927
0.0
0.520462195726
0.107334121731
0.0
0.00223498577258
0.285829022276
0.201126213914
0.353457640405
0.0
0.125919514141
0.916771112429
0.0
1.05301875868
0.694901870149
1.22899423125
0.0728751430175
0.0
0.421869052151
1.34074554362
0.0
0.0102516298071
0.0
0.0
0.220911241449
0.0
0.0
0.155305879784
0.0
0.625045657097
0.0
0.0
0.0
0.676393314327
0.0
0.0
0.304720395952
0.0
0.0
0.0
0.0
0.437860852176
0.0
0.0
0.0
0.0
0.0
0.0
0.846370287001
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.419964168324
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.11418781256
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.192257426681
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.70999271151
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.148853363228
0.0365593328524
0.485112571197
0.0
0.0
0.0
0.0
0.0
0.366330020846
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.342871125983
0.0
0.0
0.0
0.0
0.694444444444
0.0
0.0
0.015034424987
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0514107069808
0.239477179271
0.320847296347
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.368904720004
0.0
0.0
0.0
0.0
0.0
0.783149136656
0.604220688205
0.0
0.0
0.0
0.0
0.00636576632333
0.0
0.0
0.0511525900399
0.0
0.0
0.0878450077901
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.454537343354
0.0
0.0
0.0
2.24594568659
0.0
0.0
0.0
0.0
0.0
0.0
0.434673266414
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.395261264674
0.0
0.0
0.0
0.0
0.0344554062763
0.0
1.45226432929
0.0
0.997506234414
0.0
0.0376294391084
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.158795230993
0.0
0.0
0.0246188871819
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.169894044574
0.0
0.0921468084569
0.0
0.0
0.0
0.0
0.0
0.0
1.19304899419
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.102648402778
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.318421964317
0.0
0.0
0.0
0.0
0.436938772418
0.0
0.0
0.0800875839976
0.0
0.0713238313098
0.0
0.0
0.0
0.0
0.606012073974
0.851454745515
0.427513880321
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0220645809219
0.0
0.0
0.0
0.475118394391
0.0
0.0
0.13399118051
0.0
0.0844345808534
0.0
0.365729172538
0.125924733873
0.730017629147
0.120256786683
0.0
0.0
0.15474723348
0.277379640599
0.0100901173515
0.0
0.342399686635
0.0
0.0
0.0267086339582
0.0
0.0
0.118776251638
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.196250298959
0.0
0.0
0.458805471033
0.274605217298
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.578720749331
0.0
0.0
0.0
0.0
0.0
0.458882312703
0.0
0.0613485895222
0.171288735744
0.293341856832
0.0
0.0835492484831
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.024401291949
0.0
0.0
0.0
0.0
0.0
0.0
0.292184417454
0.16078609929
0.0
0.0
0.0
0.330058533818
0.0
0.0
0.0
0.0
0.10206485993
0.756111570821
0.0
0.0
0.0
0.0
0.0
0.585889869775
0.0
0.360684053153
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.72224981999
0.0
0.0
0.0
0.0
0.0
0.0
0.152342882523
0.0
0.0
0.0
0.0
0.0
0.0
0.861730416477
0.501227782456
0.0
0.0725028674819
0.0
0.0
0.0
0.0
0.732447323539
0.0160988959314
0.0
0.243264241221
0.0
0.0
0.0
0.0
0.0
0.0377015175225
0.0
0.0
0.112024634334
0.400000713077
0.0
0.0
0.0
0.0
0.14828544949
0.0
0.472675205723
0.0
0.0
0.0119335930645
0.0
0.000503354269776
0.904490147518
0.0
0.0
0.0
0.0
0.621545772284
0.0
0.0
0.0
0.0
0.557311488099
0.0
0.132481934282
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.166797639151
0.0
0.0
0.0
0.48048201221
0.0
0.0
0.0465281433652
1.273343202
0.0
0.0
0.0
0.0
0.50585988086
0.180428615992
1.57657879749
0.0529872190527
0.0
0.0
0.0
0.186107104373
0.0
0.0
0.0
0.0
0.0
0.43141907926
0.266077338347
0.0
0.165480795726
0.0
0.0
0.0
0.0
0.0
0.0223969555325
0.0
0.0
0.00992547695373
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0503355704698
0.0
0.00230529153071
0.0125465336074
0.0
0.0
0.0
0.0
0.0898921327725
0.0219702186473
0.0
0.0198617526867
0.0
0.0
0.0
0.00388939046948
0.0
0.0892878578325
0.0
0.00600379014142
0.0
0.0
0.0
0.165189851511
0.0
0.0
0.0
0.0
0.0
0.407008729654
0.120278026195
0.0
0.0
0.252847857725
0.0
0.0
0.0
0.0117406548416
0.0
0.295799621456
0.561499251511
0.0
0.0
0.0
0.0
0.504811209666
0.0158322933403
0.0
0.620599456168
0.0
0.0
0.0
0.0
0.0561183812864
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.499817567308
0.0618818818819
0.0331759666316
1.36929229951
0.0
0.148319091873
0.0
0.0
0.0
0.0
0.0
2.43440100529e-17
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0838012463775
0.0
0.0
0.23367847709
0.0
0.0
0.00916332275259
0.0866572324974
0.0
0.0
0.0241317433342
0.0
0.0
0.0
1.11878858054
0.0
0.0887689851265
0.0
0.0
0.0
1.00373771853
0.0373998078831
0.0
0.0
0.101768996058
0.0129895867939
0.0286391889462
0.0
0.189970426239
0.972675440169
0.0
0.742432459269
0.0
0.0277695385625
0.0
0.529304445307
0.117819895399
0.0
0.0
0.0
0.0
0.0
0.0
1.36131952707
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0882222045245
0.0932031448136
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.195092689346
0.145437942641
0.185543335408
0.0439300933636
0.526543777945
0.386305861333
0.0
0.0410954143426
0.0
0.0
0.0
0.616195442082
0.0
0.0
0.0563605020202
0.0
0.0
0.0
0.0
0.0
0.2961146448
0.201994697639
0.0
0.0121080333094
0.0748254559433
0.0391417358952
0.0
0.373054665992
0.829343548817
0.0
0.218192525126
0.0
0.0
0.0
0.0
0.0412633543219
0.0
0.286200772381
0.0
0.0
0.0
0.0
0.2285651497
0.0
0.0
0.0240645445463
0.0612334537359
0.0
0.0
0.0
0.0
0.128324509276
0.0159642185194
0.0
0.00412416433885
0.0
0.0
0.0
0.373988481446
0.0
0.119873328919
0.0
0.0
0.555138180207
0.133896827071
0.0
0.120518432193
0.0
1.09091856135
0.0
0.0
0.0
0.0
0.0178246242168
0.0
0.375033174098
0.0
0.0914235921629
0.0
0.0
0.0313351269749
0.0
0.0
0.0
0.0
0.0
0.0291193879571
0.0
1.18305001232
0.31286579419
0.0
0.20109968435
0.0
0.0
0.0
0.0
0.0690146212319
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0749358982473
0.0
0.207695978854
0.695274372099
0.178760351684
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0507889572756
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.241498868223
0.048518112495
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.00662810060996
0.0231160234666
0.112128851246
0.0
0.0215205808784
1.09804740797
0.350130507223
0.0
0.0
0.0
0.229224274774
0.32969957248
0.019659407154
0.0
0.00155354888824
0.581024794914
0.325144025274
0.046023044755
0.0
0.0423233543217
0.0
0.0300618236207
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0282253977064
0.462821138384
0.0
0.0678729957424
0.0
0.0
0.0
0.0
0.0
0.010694370178
0.578929833615
2.11660242291
0.0
0.0312642337786
0.0462632280893
0.0
0.0
0.0
0.0
0.220023633043
0.0
0.0
0.0
0.0
0.00154364750172
0.0
0.0156944514509
0.0298003065095
0.0600099404471
0.0
0.0118847539421
0.598098459381
0.0
0.0442171719143
0.0
0.0
0.0974568441228
0.337996722429
0.0581456897724
0.0
0.77895483971
0.24143256959
0.159832201663
1.05268613358
0.0
0.0
0.0
0.0191933077891
0.0
0.0
1.35622470992
0.0
0.344423355632
0.0
0.0
0.668219518212
0.0
0.0
0.0
0.0
0.152613301058
0.0
0.0
0.0912424424523
0.418529023866
0.0280440212217
0.0
0.0
0.0
0.0
0.54337969021
0.0015000971358
0.0
0.0
0.0
0.0
0.190026208415
0.0
0.0
0.0259408694743
1.03278471809
0.0
0.0
0.0
0.0323925459375
0.227333192166
0.0
0.0
0.04006727635
0.648841591694
0.0
0.323004883878
0.0698304810671
0.0
0.0
0.0
0.0
0.0
0.00105892272788
0.0
0.0
0.0
0.0
0.0191184352926
0.0
0.1631495511
0.756143667297
0.0
0.0
0.0
0.0
0.0
0.163050084897
0.0233334183873
0.0
0.0
0.0
0.0
0.0
0.0415457625653
0.0
0.0
0.0
0.0
0.142875356263
0.0
0.0
0.0421005527134
1.9568324757
0.087220529452
0.0
0.0
0.0915332149585
0.0
0.82381992271
0.0307683613336
1.28818108525
0.788569164479
0.59213555385
0.0
0.0
0.0
0.0
0.0
0.0385138295765
0.0
0.0
0.0
0.0
0.049005049005
0.0
0.0
0.0796205970931
0.0
0.0
0.0
0.0
0.0
0.735651750973
1.55920600049
0.0
0.0
0.32956817741
0.0
0.414218230081
0.147440157224
0.0
0.0
0.0
0.0
0.0
0.595054239622
0.0409855892665
0.0
0.313752603249
0.0
0.1033121991
0.0138414446998
0.0
0.0
0.0328889041417
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0158974509968
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.227848217483
0.0
0.22474672098
0.0
0.0846723481032
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.682521606334
0.150947229317
0.0
0.0
0.380961081674
0.605403988638
0.253952422026
0.0293833924111
0.0
0.0
0.0
0.46875
0.0
0.0
0.101267310225
0.0436619398409
0.0
0.0
0.0
0.0
0.0
0.151826245102
0.0
0.0
1.50253118402
0.0980232955847
0.165345821326
0.0
0.301360351481
0.391414141414
0.0
0.0
0.0
0.0
0.210178898367
0.0
0.0
0.0
0.471739890456
0.0
0.431656027037
0.151110346617
0.058623048508
0.0
0.0
0.0913079376386
0.0
0.0
0.0557656993717
0.0
0.0
0.102717098709
0.0
0.0
0.157514076018
0.111690829959
0.0402915509718
0.0
0.0
0.0
0.0
0.125045515785
0.0641938908326
0.541380968893
0.0976659659819
0.0
0.0
0.409918660123
0.0293380267852
0.0
0.0
0.0377368323008
0.0
0.0
0.0
0.126877668745
0.0
0.0
0.0
0.0
0.465905136546
0.0
0.0
0.39964786727
1.08725780349
0.0
0.0
0.0
0.0
0.0
0.0637266527554
0.0
0.0
0.0
0.0165594658355
0.0
0.0
0.461835721067
0.0
0.0
0.327601507644
0.0
0.0
0.457744745394
0.646622390736
0.0
0.0
0.0
0.0839810473308
0.0
0.0
0.0
0.0
0.0
0.0
0.274381200356
0.0453532781668
0.0
0.0
0.0
0.354754471969
0.0
0.0967428002892
0.438081062769
0.0
0.306746053594
0.0
0.0
0.113440246006
0.0
0.0
0.260764742116
0.0
0.517776530142
0.0
0.0
0.0
0.0
0.0824519569029
0.0
0.0
0.239474199297
0.775382205388
0.561262866691
0.256101506057
0.0
0.0
0.0
0.0149638527237
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.120283054837
0.468162470061
0.00536586923888
0.0
0.157956058589
0.0134600650225
0.0
0.0
0.0
0.0
0.133233169093
0.0
0.419665062223
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.192907386412
0.0
0.0663832675279
0.0
0.798290053259
0.0
0.316082312217
0.0
0.0
0.0863966882771
0.0
0.0
0.0
0.0
0.0
0.0130958929702
0.0
1.23691518403
0.0
0.0
0.0
0.0
0.0362691355017
0.0154182487631
1.84843791319
0.464035981588
0.0
0.0
0.0
0.17491470132
0.0
0.0688188202533
0.0
0.0
0.474439079126
0.0
0.0
0.0
0.0
0.0122203893724
0.0533283269526
0.0
0.0
0.0
0.00685194465692
0.0236139575934
0.0
0.173268206583
0.0325329064565
0.0
0.310217717484
0.0
0.0583929077162
0.0
0.0
0.0
0.0
0.0725849669997
0.0
0.362319769496
0.0
0.00738933241955
0.0
0.0
0.0
0.151979622378
0.204414446412
0.0
0.0
0.169455476308
0.0
0.0
0.264850327409
0.0
0.0
0.0
0.0
0.0
0.0
0.267950963222
0.0
0.316034007776
0.0
0.0
0.0
0.0
0.0
0.0997854077253
0.0
0.0
0.036405812091
0.0
0.0
0.0
0.692290811048
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.375581296782
0.0
0.270864139523
0.0
0.0731634689969
0.0
0.0
0.0
0.188140636027
0.816861749117
0.0
0.539101497504
0.0
0.0
1.19260390602
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.00808665753212
0.0
0.0
0.0
0.0
0.0
0.242926478408
0.0543385490754
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.297127264458
0.263388937665
0.0
0.361046527165
0.0710396502663
2.06107244388
0.193964990999
0.0
0.0439836563463
0.0271441877832
1.21154656553
0.0
0.0
0.0
0.0321990395435
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0408267826004
0.119021711496
0.0
0.0
0.100450429999
0.507060432384
0.0
0.0
0.0764455644345
0.0
0.0
0.0
0.164748995641
0.362380310495
0.19210279089
0.0
0.0
0.0
0.0
0.0
0.0681269609787
0.0
0.0
0.0
1.93955722911
0.0359407590848
0.0
0.0
0.0
0.0
0.684550886425
0.312078154109
0.106338270091
0.528306450726
0.0
0.0
0.178176331106
0.0
0.19510902613
0.0
0.205297734355
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0317129780786
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
1.34027014071
0.0
0.230324266404
0.0
0.297934322034
0.0
0.0
0.0
0.0
0.0
0.0
0.045893427958
0.0519524924173
0.376661742984
0.0
0.0
0.0
0.0491166917531
0.0
0.0450802528892
0.0
0.568904991091
0.0
0.0
0.0
0.0335247463923
0.543948316277
0.0
0.0
0.456341945616
1.19562138281
0.19863436721
0.484655120024
0.0
0.0
0.0184197425503
0.0
0.0
0.17428202731
0.200370605472
0.160931146955
0.0
0.114780914185
0.0
0.32297507864
0.0
0.0
0.0284035539246
0.0
1.53964048687
0.0
0.386512118019
0.0
0.0
0.495536433659
0.273577427737
0.2214305397
0.0
0.0
0.219131644619
1.09456043347
0.0
0.0
0.0
0.0
0.0
0.0
0.0255617781702
0.0
0.110201401762
0.0
0.101433260786
0.0
0.247261787242
0.0
0.315039780108
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0389843972482
0.0
0.128026093495
0.0847296172628
0.0
0.0
0.0
0.0
0.0535431799839
0.0
0.0
0.0
0.0
0.0
0.0861575761736
0.0
0.0
0.0
0.0382415789519
0.0
0.0
0.0966499288016
0.00789381575638
0.0
0.207023243222
0.0
0.0
0.0
0.105054333258
0.0
0.0
0.0
0.0762289518147
0.0
0.0
0.0
0.00554425612053
0.0
0.520779905147
0.0
0.0
1.04923588419
0.0
0.133448873484
0.0
0.0
0.331537528168
0.114424147071
0.0666637456047
0.0
0.235951700775
1.29983541369
0.0
0.0201406798802
0.0
0.0710825803709
0.0
0.0
0.0
0.0409526409967
0.060420537332
0.120534048472
0.0235223564166
0.0
0.0
0.0
0.147288829459
0.0381988408382
0.0177354743726
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.085422695528
0.251533442697
0.0
0.0
0.77819488266
0.125232695396
0.683245666983
0.0
0.0
0.0
0.0
0.0
1.2881178172
0.572716123328
0.0
0.0
0.0
0.0
0.0
0.0
0.036815553949
0.0
0.0499062081497
0.0
0.0792578630187
0.554263686065
0.0
0.0
0.514476176742
0.0224006156894
0.0
0.0
0.0
0.0
0.0840001526582
0.0
1.0436471311
0.0
0.0
0.0
0.0216237264741
0.0
0.0
0.0
0.322840597108
0.783215318222
0.0
0.0
0.110007927226
0.860714285714
0.3494767156
0.47517523703
0.0
0.0
0.0
0.0
0.0
0.0
0.756301033235
0.0
0.19682333439
0.200815755739
0.0
0.0653336091834
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
2.37399815043
0.294914975
0.44607244471
0.0
0.0968827630419
0.0333902098218
0.0
0.0
0.0131629659427
0.108380416678
0.507294482255
0.0
0.349744019591
0.0
0.0
0.149519395006
0.508507971949
0.0
0.0
0.0611777479014
0.0
0.0
0.0
0.0
1.91372040424
0.0937806159918
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.249995167619
0.0
0.0
0.0
0.0
0.0
0.475716102162
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.637224633283
1.24576857278
0.153777935438
0.0990901336366
0.999410184154
0.0
0.0
0.0
1.54020539869
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.629801604052
0.0
0.0
1.2427311095
0.154514645063
0.0
0.159194032146
0.0
0.0
0.0
0.0
0.139717038293
0.0
0.0
0.336861165746
0.194155748901
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.115029300377
0.0
0.139128316933
0.00214100904059
0.0
0.796638119049
0.0
0.0422477629055
0.0
0.0
0.291839593962
0.0902206960463
0.0
0.0
0.0662174880486
0.0
0.0
0.0
0.0
0.0
0.41738772378
0.199084822938
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.84102068317
0.0
0.0257949204092
0.0
eval
0.68085106383
0.0468521229868
0.4986829763
0.114924532037
AP for roost = 0.2384
Mean Precision for roost = 0.1149
Mean Recall for roost = 0.4987
Mean AP = 0.2384
~~~~~~~~
Results:
0.238
0.238
~~~~~~~~

--------------------------------------------------------------
Results computed with the **unofficial** Python eval code.
Results should be very close to the official MATLAB eval code.
Recompute with `./tools/reval.py --matlab ...` for your paper.
-- Thanks, The Management
--------------------------------------------------------------

real	0m11.288s
user	0m12.035s
sys	0m1.457s
